{"cells":[{"cell_type":"markdown","source":["# Spark Exercise #2 - Wikipedia\n\n### Setup \nTo start, first download the data for the assignment (133 MB):\n[wikipedia.dat](http://alaska.epfl.ch/~dockermoocs/bigdata/wikipedia.dat) and upload it to Databricks.\n\n### The Assignment\n\nIn this assignment, you will get to know Spark by exploring full-text Wikipedia articles.\n\nGauging how popular a programming language is important for companies judging whether or not they should adopt an emerging programming language. \nFor that reason, industry analyst firm RedMonk has bi-annually computed a ranking of programming language popularity using a variety of data sources, \ntypically from websites like GitHub and StackOverflow. \nSee their [top-20 ranking for June 2016](https://redmonk.com/sogrady/2016/07/20/language-rankings-6-16/) as an example.\n\nIn this assignment, we'll use our full-text data from Wikipedia to produce a rudimentary metric of how popular a programming language is, \nin an effort to see if our Wikipedia-based rankings bear any relation to the popular Red Monk rankings.\n\nTry to look into [PySpark Docs](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) whenever you need help"],"metadata":{}},{"cell_type":"markdown","source":["### Data Exploration\nFirst step, we will get to know the data. Read it as a Spark RDD, try to understand it and get some insights about it."],"metadata":{}},{"cell_type":"code","source":["rdd = sc.textFile('/FileStore/tables/wikipedia.dat')\nsample = rdd.take(1)[0]\nsample"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Play with the sample and with the RDD here to get more insights.\n# For example, how many articles are in the dataset?\nrdd.count()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### Read-in Wikipedia Data\nFrom the sample we can see how each articles is composed. Each one looks like an HTML string.\n\nOur job now is to parse this data into a PairRDD with the article's title as the key and its text as the value."],"metadata":{}},{"cell_type":"code","source":["tags = \"</title><text>\"\ndef parse_wikipedia_article(line):\n  i = line.index(tags)\n  title = line[14:i]\n  text = line[i+len(tags): len(line)-16]\n  return (title, text)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["rdd_wiki_articles = rdd.map(lambda l: parse_wikipedia_article(l))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Compute a ranking of programming languages\nWe will use a simple metric for determining the popularity of a programming language: the number of Wikipedia articles that mention the language at least once.\n\n### Rank languages attempt #1: Count appearances\nFor each language in `LANGS` list, count in how many articles it appears.\n\nThe list should be sorted in descending order. That is, according to this ranking, the pair with the highest second component (the count) should be the first element of the list.\n\nPay attention to roughly how long it takes to run this part! (It should take tens of seconds.)\n\nExample of the output:\n```\n[('javascript', 1721), ('c#', 706), ('java', 618), ...]\n```"],"metadata":{}},{"cell_type":"code","source":["LANGS = [\"javascript\", \"java\", \"php\", \"python\", \"c#\", \"c++\", \"ruby\", \"css\", \"objective-c\", \"perl\", \"scala\", \"haskell\", \"matlab\", \"clojure\", \"groovy\"]"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Write your code here\n# Option 1 - For each language, filter the whole RDD\nresults = []\nfor l in LANGS:\n  counter = rdd_wiki_articles.filter(lambda article: l in article[1].lower().split(' ')).count()\n  results.append((l, counter))\nsorted(results, key=lambda x: x[1], reverse=True)\n\n# Option 2 - For each article, count all languages - Better performance since it doesn't read the whole RDD many times\ndef count_langs(line):\n  lang_dict = {}\n  for l in LANGS:\n    if l in line.lower().split(' '):\n      lang_dict[l] = 1\n  return lang_dict\n\nresults2 = rdd_wiki_articles\\\n            .map(lambda article: count_langs(article[1]))\\\n            .filter(lambda d: len(d)>0)\\\n            .reduce(lambda d1, d2: { key: d1.get(key, 0) + d2.get(key, 0) for key in set(d1) | set(d2) })\nsorted(results2.items(), key=lambda x: x[1], reverse=True)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Rank languages attempt #2: Compute an inverted index\n\nAn inverted index is an index data structure storing a mapping from content, such as words or numbers, to a set of documents. In particular, the purpose of an inverted index is to allow fast full text searches. In our use-case, an inverted index would be useful for mapping from the names of programming languages to the collection of Wikipedia articles that mention the name at least once.\n\nTo make working with the dataset more efficient and more convenient, implement a method that computes an \"inverted index\" which maps programming language names to the Wikipedia articles on which they occur at least once.\n\nHint: You might want to use methods flatMap and groupByKey on RDD for this part.\n\n\n#### Computing the ranking\n\nLike in part 1, you should compute a list of pairs where the second component of the pair is the number of articles that mention the language (the first component of the pair is the name of the language). This time, you'll use the inverted index created above.\n\nAgain, the list should be sorted in descending order. That is, according to this ranking, the pair with the highest second component (the count) should be the first element of the list.\n\nHint: method mapValues on PairRDD could be useful for this part.\n\nCan you notice a performance improvement over attempt #2? Why?"],"metadata":{}},{"cell_type":"code","source":["# Write your code here\n# There are many ways of solving this:\n# 1) You can create an rdd with the mapping {word -> list of articles}\n# 2) Also an rdd but with the mapping {word -> amount of articles}\n# Afterwards you can collect the rdd into memory and find the relevant languages with python.\n# Possible improvements:\n# A) To improve this, you could filter the rdds before the collect() so they contain only the languages.\n# B) Use reduceByKey instead of groupByKey()\n\n# Option 1 - Group By Key and create a list\ninverted_index = rdd_wiki_articles\\\n  .map(lambda article: (article[0].lower(), list(set(article[1].lower().split(' ')))))\\\n  .flatMap(lambda art: [(word, art[0]) for word in art[1] if word != ''])\\\n  .groupByKey()\\\n  .mapValues(list)\\\n  .collect()\ninverted_index\n\n# Option 2 - Group By Key and calculate values's size\n# inverted_index = rdd_wiki_articles\\\n#   .map(lambda article: (article[0].lower(), list(set(article[1].lower().split(' ')))))\\\n#   .flatMap(lambda art: [(word, art[0]) for word in art[1] if word != ''])\\\n#   .groupByKey()\\\n#   .mapValues(len)\\\n#   .collect()\n# inverted_index\n\n# Option 1 with improvement B - ReduceByKey\n# inverted_index = rdd_wiki_articles\\\n#   .map(lambda article: (article[0].lower(), list(set(article[1].lower().split(' ')))))\\\n#   .flatMap(lambda art: [(word, art[0]) for word in art[1] if word != ''])\\\n#   .reduceByKey(lambda a,b: (a if type(a) == list else [a]) + (b if type(b) == list else [b]))\\\n#   .collect()\n# inverted_index\n#  .reduceByKey(lambda a,b: (a if type(a) == list else [a]) + (b if type(b) == list else [b]))\\\nresults = {l:0 for l in LANGS}\nfor word, articles in inverted_index:\n    if word in LANGS:\n      results[word] += len(articles) if type(articles) == list else 1\nresults = [(lang, counter) for lang, counter in results.items()]\nsorted(results, key=lambda x: x[1], reverse=True)"],"metadata":{},"outputs":[],"execution_count":12}],"metadata":{"name":"02_Spark_Wikipedia_solution","notebookId":2701358843967854},"nbformat":4,"nbformat_minor":0}
