{"cells":[{"cell_type":"markdown","source":["# Exercise #3 - Twitter\n\nIn this homework, we are going to play with Twitter data.\n\nThe data is represented as rows of of [JSON](https://en.wikipedia.org/wiki/JSON#Example) strings.\nIt consists of [tweets](https://dev.twitter.com/overview/api/tweets), [messages](https://dev.twitter.com/streaming/overview/messages-types), and a small amount of broken data (cannot be parsed as JSON).\n\nFor this exercise, we will only focus on tweets and ignore all other messages.\n\n\n## Tweets\n\nA tweet consists of many data fields. [Here is an example](https://gist.github.com/arapat/03d02c9b327e6ff3f6c3c5c602eeaf8b). You can learn all about them in the Twitter API doc. We are going to briefly introduce only the data fields that will be used in this exercise.\n\n* `created_at`: Posted time of this tweet (time zone is included)\n* `id_str`: Tweet ID - we recommend using `id_str` over using `id` as Tweet IDs, becauase `id` is an integer and may bring some overflow problems.\n* `text`: Tweet content\n* `user`: A JSON object for information about the author of the tweet\n    * `id_str`: User ID\n    * `name`: User name (may contain spaces)\n    * `screen_name`: User screen name (no spaces)\n* `retweeted_status`: A JSON object for information about the retweeted tweet (i.e. this tweet is not original but retweeteed some other tweet)\n    * All data fields of a tweet except `retweeted_status`\n* `entities`: A JSON object for all entities in this tweet\n    * `hashtags`: An array for all the hashtags that are mentioned in this tweet\n    * `urls`: An array for all the URLs that are mentioned in this tweet\n\n\n## Data source\n\nAll tweets are collected using the [Twitter Streaming API](https://dev.twitter.com/streaming/overview).\nWe provide you with a file `tweets_10mb.txt`. You should upload it to Databricks."],"metadata":{}},{"cell_type":"markdown","source":["## Data Exploration\nLet's see how many lines there are in the input files.\n\n1. Load the data from `tweets_10mb.txt` into an RDD.\n2. Mark the RDD to be cached (so in next operation data will be loaded in memory) \n3. Try to understand the data and get some insights about it. How many tweets are in the dataset?"],"metadata":{}},{"cell_type":"code","source":["# Write your code here\nrdd = sc.textFile('/FileStore/tables/tweets_10mb.txt').persist()\nprint(\"Total amount of tweets: {}\".format(rdd.count()))\nsample = rdd.take(1)[0]\nprint(sample)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Part 1: Parse JSON strings to JSON objects\nPython has built-in support for JSON, look at this example:"],"metadata":{}},{"cell_type":"code","source":["import json\n\njson_example = '''\n{\n    \"id\": 1,\n    \"name\": \"A green door\",\n    \"price\": 12.50,\n    \"tags\": [\"home\", \"green\"]\n}\n'''\n\njson_obj = json.loads(json_example)\n'price' in json_obj.keys()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## Broken tweets and irrelevant messages\n\nThe data of this assignment may contain broken tweets (invalid JSON strings). So make sure that your code is robust for such cases.\n\nYou can filter out such broken tweet by checking if:\n* the line is not in json format\n\nIn addition, some lines in the input file might not be tweets, but messages that the Twitter server sent to the developer (such as [limit notices](https://dev.twitter.com/streaming/overview/messages-types#limit_notices)). Your program should also ignore these messages.\n\nThese messages would not contain the `created_at` field and can be filtered out accordingly.\n* Check if json object of the broken tweet has a `created_at` field\n\n*Hint:* [Catch the ValueError](http://stackoverflow.com/questions/11294535/verify-if-a-string-is-json-in-python)\n\n**********************************************************************************\n\n### Task 1\nParse raw JSON tweets to obtain valid JSON objects. \n\n### Task 2\nFrom all valid tweets, construct a pair RDD of `(user_id, text)`, where `user_id` is the `id_str` data field of the `user` dictionary (read [Tweets](#Tweets) section above), `text` is the `text` data field."],"metadata":{}},{"cell_type":"code","source":["# Task 1 - Write your code here\nimport json\n\ndef safe_parse(raw_json):\n    \"\"\"\n    Input is a String\n    Output is a JSON object if the tweet is valid and None if not valid\n    \"\"\"\n    ret_val = None\n    try:\n      ret_val = json.loads(raw_json)\n    except Exception:\n      ret_val = None\n    return ret_val\n\nrdd_parsed = rdd.map(lambda r: safe_parse(r)).persist()\nrdd_valid_tweets = rdd_parsed.filter(lambda j: 'created_at' in j.keys()).persist()\nprint(\"Valid Tweets: {}\".format(rdd_valid_tweets.count()))"],"metadata":{"collapsed":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Task 2 - Write your code here\nrdd_key_val = rdd_valid_tweets.map(lambda t: (t['user']['id_str'], t['text'])).persist()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### Task 3 - Number of unique users\n\nCount the number of different users in all valid tweets\n\nHint: [the `distinct()` method](https://spark.apache.org/docs/latest/programming-guide.html#transformations) is an easy way to do this, but try to see if there is a another way to do this."],"metadata":{}},{"cell_type":"code","source":["# Task 3 - Write your code here\nusers_count = rdd_key_val.map(lambda user_and_tweet: user_and_tweet[0]).distinct().count()\nprint('The number of unique users is:', users_count)  # Should print 1748\n\n# Another solution:\nfrom operator import add\nusers_count = rdd_key_val.map(lambda user_and_tweet: user_and_tweet[0]).groupBy(lambda x: x).count()\nprint('The number of unique users is:', users_count)  # Should print 1748"],"metadata":{"collapsed":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## Part 2:  Words popularity\nIn this task you'll need to find the top-20 used words in all the dataset.\nThe output should be a list of tuples <word, # of appearances in different tweets>\nAnd should be sorted by the number of appearances of the word.\n\nBelow is a list of `Stop Words`. Those are the most common words in english and you should remove them from every tweet so we don't calculate their appearance (they are not intereseting in our case)."],"metadata":{"collapsed":true}},{"cell_type":"code","source":["STOP_WORDS = [\"\", \"-\",\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"],"metadata":{"collapsed":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Write your code here\nrdd_popular_words = rdd_key_val\\\n                      .flatMap(lambda p: p[1].split(\" \"))\\\n                      .map(lambda w: (w,1))\\\n                      .reduceByKey(lambda a,b: a+b)\\\n                      .filter(lambda p: p[0].lower() not in STOP_WORDS)\\\n                      .filter(lambda p: p[1]> 50)\\\n                      .sortBy(lambda v: -v[1])\\\n                      .persist()\ntop_20 = rdd_popular_words.take(20)\ntop_20"],"metadata":{"collapsed":true},"outputs":[],"execution_count":13}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.5.2","nbconvert_exporter":"python","file_extension":".py"},"name":"HW-2","notebookId":3148839095156136},"nbformat":4,"nbformat_minor":0}
