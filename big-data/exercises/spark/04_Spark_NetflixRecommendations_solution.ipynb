{"cells":[{"cell_type":"markdown","source":["# Exercise #4 - Netflix Recommendations\nAs you remember, we implemented several MapReduce algorithms on the `MovieLens` dataset.\nYou will need to solve the same issues using Spark."],"metadata":{}},{"cell_type":"markdown","source":["## Task 1 - Ranking Breakdown\nDevelop a Spark application to know how many movies got a rating of 5, 4, 3, 2 and 1.\n\nUse as input the file `FileStore/tables/ratings.csv`.\n\nRemember: \n1. If a rating has a decimal, round it up. For example: 4.5 will be rounded to 5.0.\n2. Remove the word 'rating' from the loaded data."],"metadata":{}},{"cell_type":"code","source":["# Write your code here\nimport math\nfrom operator import add\n\ndef parse_line(line):\n  (userID, movieID, rating, timestamp) = line.split(',')\n  if rating != 'rating':\n    return [(math.ceil(float(rating)), 1)]\n  else:\n    return []\n\nsc.textFile('FileStore/tables/ratings.csv')\\\n  .flatMap(parse_line)\\\n  .reduceByKey(add)\\\n  .takeOrdered(5)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Task 2 - Movies By Popularity\n1. Write a Spark job that ranks movies (movie ID) by their popularity.\n2. Print the top 20 movies (movieID, average rating).\n3. Show only movies with more than 10 rankings."],"metadata":{}},{"cell_type":"code","source":["# Write your code here\nimport math\nfrom operator import add\n\ndef parse_line(line):\n  (userID, movieID, rating, timestamp) = line.split(',')\n  if rating != 'rating':\n    return [(movieID, [math.ceil(float(rating))])]\n  else:\n    return []\n\nsc.textFile('FileStore/tables/ratings.csv')\\\n  .flatMap(parse_line)\\\n  .reduceByKey(add)\\\n  .mapValues(lambda ratings: sum(ratings)/len(ratings) if len(ratings)> 10 else 0)\\\n  .takeOrdered(20, key=lambda x: -x[1])"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## Task 3 - Netflix Recommendations\nThis task is similar to Exercise #3 in MapReduce.\n\nFor each movie, find the top 10 movies with highest similarity (and their 'similarity value')\n\nUse the file: `/FileStore/tables/ratings_small.csv`.\n\nThis time there's no need for you to merge the files nor build a table from the output."],"metadata":{}},{"cell_type":"code","source":["# Write your code here\nimport math\nfrom operator import add\n\ndef parse_line(line):\n  (userID, movieID, rating, timestamp) = line.split(',')\n  if rating != 'rating':\n    return [(userID, [int(movieID)])]\n  else:\n    return []\n\ndef movies_permutations(user_movies_pair):\n  permutations = []\n  user, movies = user_movies_pair\n  for i in range(len(movies)):\n    for j in range(i+1, len(movies)):\n      m1 = movies[i]\n      m2 = movies[j]\n      if m1 <= m2:\n        permutations.append(((m1,m2), 1))\n      else:\n        permutations.append(((m2,m1),1))\n  return permutations\n  \ndef sim_between_two_movies(movies_pair_and_simval):\n  pair, simval = movies_pair_and_simval\n  m1, m2 = pair\n  return [(m1, [(m2, simval)]), (m2, [(m1, simval)])]\n  \ndef top_10_similar_movies(movie2_simval_values): # This might be dangerous if the amount of values is big, can you think about other solution?\n  # This can be improved by using a MaxHeap of size 10 so we don't load all the list into memory.\n  sorted_pairs = sorted(movie2_simval_values, key=lambda x: x[1], reverse=True)\n  top_10 = sorted_pairs[:min(len(sorted_pairs), 10)]\n  \n  return top_10\n  \nsc.textFile('FileStore/tables/ratings_small.csv')\\\n  .flatMap(parse_line)\\\n  .reduceByKey(add)\\\n  .flatMap(movies_permutations)\\\n  .reduceByKey(add)\\\n  .flatMap(sim_between_two_movies)\\\n  .reduceByKey(add)\\\n  .mapValues(top_10_similar_movies)\\\n  .take(20)"],"metadata":{},"outputs":[],"execution_count":7}],"metadata":{"name":"04_Spark_NetflixRecommendations_solution","notebookId":1403255367755185},"nbformat":4,"nbformat_minor":0}
