{"cells":[{"cell_type":"markdown","source":["# Exercise #1 - Getting Started with Spark SQL\nIn this exercise you will play with some basic SparkSQL functions"],"metadata":{}},{"cell_type":"markdown","source":["## Task 1 - Reading data into a DataFrame\n1. Load the following path: `\"/databricks-datasets/samples/population-vs-price/data_geo.csv\"` into a Dataframe.\n2. Print how many rows are in the dataset\n3. Drop rows with missing values and count again how many rows are left.\n4. Show the first 10 lines of the DataFrame\n5. Use databricks's `display(df)` instead of `df.show()` to view this data in a tabular format."],"metadata":{}},{"cell_type":"code","source":["# Write your code here\n# 1\ndf = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(\"/databricks-datasets/samples/population-vs-price/data_geo.csv\")\n\n# What's the purpose of the \"inferSchema\" option? Hint: Use df.printSchema() to find out.\n\n# 2\nprint(\"There are {} rows\".format(df.count()))\n\n# 3\ndf = df.dropna()\nprint(\"There are {} rows after dropping rows with missing values\".format(df.count()))\n\n# 4\ndf.show(10)  # First try this, then the line below\n\n# 5\n#display(df)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Task 2 - Basic SQL on Dataframes\n1. Select the `State`, `2014 Population estimate` and `2015 median sales price` fields from the dataframe.\n2. Add the condition `where 'State'='Alabama'`."],"metadata":{}},{"cell_type":"code","source":["# Write your code here\n\n# 1 - There are some ways of doing this\n# Option 1\ndf_states = df.select(\"State\", \"2014 Population estimate\", \"2015 median sales price\")\ndf_states = df.select(df[\"State\"], df[\"2014 Population estimate\"], df[\"2015 median sales price\"])\ndf_states.show(2)\n\n# 2 - Again, There are some ways of doing this. Filter() and Where() have the same functionality.\ndf_alabama = df_states.where(\"State = 'Alabama'\")\ndf_alabama = df_states.filter(df.State == 'Alabama')\ndf_alabama.show(2)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## Task 3 - Simple Transformations\n1. Load the file `\"/databricks-datasets/iot/iot_devices.json\"` into a Dataframe.\n2. Filter out all devices whose temperature exceed 25 degrees.\n3. Keep the columns: `device_name`, `temp`, `humidity` and `cca3`.\n4. Group the rows by `cca3` and compute the average temp and humidity. Rename the last two columns to be: `average_temp` and `average_humidity`.\n5. Sort the df from step 4 by humidity, descending.\n6. Using the df from step 3, group the rows by `cca3` but this time compute the min temp, max humidity and total count of rows."],"metadata":{}},{"cell_type":"code","source":["# Write your code here\nfrom pyspark.sql import functions as F\n\n# 1\ndf = spark.read.json(\"/databricks-datasets/iot/iot_devices.json\")\n\n# 2\ndf = df.filter(df.temp <= 25)\n\n# 3\ndf = df.select(\"device_name\", \"temp\", \"humidity\", \"cca3\")\ndf.show(5)\n\n# 4\n# Option 1\ndf_avg = df\\\n      .groupBy(\"cca3\")\\\n      .avg()\\\n      .withColumnRenamed(\"avg(temp)\", \"average_temp\")\\\n      .withColumnRenamed(\"avg(humidity)\", \"average_humidity\")\n\n# Option 2:\ndf_avg = df\\\n      .groupBy(\"cca3\")\\\n      .agg(F.avg(\"temp\").alias(\"average_temp\"), \n           F.avg(\"humidity\").alias(\"average_humidity\"))\n\n# 5\ndf_avg = df_avg.sort(\"average_humidity\", ascending=False)\ndf_avg.show(5)\n\n# 6\n# Option 1\ndf_min_max = df\\\n      .groupBy(\"cca3\")\\\n      .agg({\"temp\": \"min\", \"humidity\": \"max\", \"*\": \"count\"})\n\n# Option 2\ndf_min_max = df\\\n      .groupBy(\"cca3\")\\\n      .agg(F.min(\"temp\"), F.max(\"humidity\"), F.count(\"*\"))\n\ndf_min_max.show(5)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Task 4 - Pets Dataframe\n1. Create a data frame from the data below, it should have a schema with the right types\n```\nid,breed_id,nickname,birthday,age,color,weight\n1,1,\"King\",\"2014-11-22 12:30:31\",5,\"brown\",10.0\n2,3,\"Argus\",\"2016-11-22 10:05:10\",10,None,5.5\n3,1,\"Chewie\",\"2016-11-22 10:05:10\",15,None,12.0\n3,2,\"Maple\",\"2018-11-22 10:05:10\",17,\"white\",3.4\n4,2,None,\"2019-01-01 10:05:10\",13,None,10.0\n```\n\nHint: The `birthday` field should be of type `TimestampType()`. Therefore you'll need to use python's `datetime.strptime` method on the date strings to get datetime objects, for example: `datetime.strptime(\"2014-11-22 12:30:31\", '%Y-%m-%d %H:%M:%S')`"],"metadata":{}},{"cell_type":"code","source":["# Write your code here\nfrom pyspark.sql import types as T\nfrom datetime import datetime\n\nschema = T.StructType([\n    # Fill this array with all the fields\n    T.StructField(\"id\", T.IntegerType(), False),\n    T.StructField(\"breed\", T.StringType(), True),\n    T.StructField(\"nickname\", T.StringType(), True),\n    T.StructField(\"birthday\", T.TimestampType(), True),\n    T.StructField(\"age\", T.IntegerType(), True),\n    T.StructField(\"color\", T.StringType(), True),\n    T.StructField(\"weight\", T.DoubleType(), True),\n])\n\ndata = [\n  # Fill this array with all the data\n  (1,1,\"King\",datetime.strptime(\"2014-11-22 12:30:31\", '%Y-%m-%d %H:%M:%S'),5,\"brown\",10.0),\n  (2,3,\"Argus\",datetime.strptime(\"2016-11-22 10:05:10\", '%Y-%m-%d %H:%M:%S'),10,None,5.5),\n  (3,1,\"Chewie\",datetime.strptime(\"2016-11-22 10:05:10\", '%Y-%m-%d %H:%M:%S'),15,None,12.0),\n  (3,2,\"Maple\",datetime.strptime(\"2018-11-22 10:05:10\", '%Y-%m-%d %H:%M:%S'),17,\"white\",3.4),\n  (4,2,None,datetime.strptime(\"2019-01-01 10:05:10\", '%Y-%m-%d %H:%M:%S'),13,None,10.0),\n]\n\n# df_pets = # Write your code here\ndf_pets = spark.createDataFrame( \n    data=data,\n    schema=schema\n)\n\ndf_pets.show()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## Task 5 - Manipulations on the Pets Dataframe\nFor the following steps, use the `df_pets` created on the previous task\n1. Create a new Dataframe `df_pets2` by changing/adding the following columns:\n  1. Add `birthday_date` - a new column of DateType type, casted from the `birthday` column.\n  2. Add `owned_by` - a new column with the literal `'me'`. (In other words, the word `'me'` will appear in every row).\n  3. Rename the `pet` column to `pet_id`.\n2. Filter the DF such that only pets born after `January 1st, 2017` are on the table."],"metadata":{}},{"cell_type":"code","source":["# Write your code here\nfrom pyspark.sql import functions as F\n\n# 1\ndf_pets2 = df_pets\\\n            .withColumn('birthday_date', F.col('birthday').cast('date'))\\\n            .withColumn('owned_by', F.lit('me'))\\\n            .withColumnRenamed('id', 'pet_id')\n\n# 2\ndf_pets2 = df_pets2.where(F.col('birthday_date') > datetime(2017,1,1))\ndf_pets2.show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## Task 6 - Array, Map and Fill\nFor the following steps, create a new df and use the `df_pets` created on task 4\n1. Fill rows with missing `nickname` and `color` with a default value `Unknown Name` and `Unknow Color` respectively. Hint: Use `df.fillna({...})`.\n2. Add a new column `double_age` that is the result of multiplying the pet's age by 2.\n3. Add a new column `array_column` to the df. It should contain an array with the literal `\"array\"`, `today's datetime`, the `breed` and the `double_age` column.\n4. Add a new column `mapping`. It should be a map with the literal `AGE` as key and `age` as value. Hint: Use `F.create_map().`\n5. Add a new column `math_operation`. You should take the 4th element in the array inside the `array_column` (the `double age`), minus the value of the mapping in column `mapping` (the `age`) and multiply the result by the `id`. The result should be of type `int`."],"metadata":{}},{"cell_type":"code","source":["# Write your code here\nfrom pyspark.sql import functions as F\n# 1\ndf = df_pets\\\n  .fillna({\n      'nickname': 'Unknown Name',\n      'color':      'Unknown Color',\n    })\n\n# 2\ndf = df.withColumn('double_age', df.age * 2)\n\n# 3\ndf = df\\\n      .withColumn('array_column', F.array([\n        F.lit(\"array\"),\n        F.lit(datetime.now()),\n        df.breed,\n        df.double_age\n      ]))\n\n# 4\ndf = df.withColumn(\"mapping\", F.create_map(F.lit(\"AGE\"), \"age\"))\n\n# 5\ndf.withColumn(\"math_operation\", ((df.array_column.getItem(3)-df.mapping.AGE)*df.id).cast('int')).show()\n# df.show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## Task 7 - UDF\nSometimes you can't find functions that can perform what you want. For this situations you might want to implement your own UDF (User Defined Function).\n\nThese are functions written in python code that take a subset of columns as the input and returns a new column back.\n\nYour task is to write a UDF that will convert all chars in a given string to UPPERCASE and will replace every 's' with a '$', every 'a' with a '@' adn every i with a '!'.\n\n1. Write a method in python that receives a string. It should convert all its chars to `UPPERCASE` and replace every `'S'` with a `'$'`, every `'A'` with a `'@'`, every `'I'` with a `'!'` and every `'O'` with `'0'`.\n2. Register the UDF as a spark udf. It should return a column of type `StringType()`.\n3. Apply the UDF to the columns `nickname` and `color` from the `df_pets` built on task 4. Apply it another time to the `nickname` column but with `max_length=2`. Hint: Use a literal to send the `max_length` param."],"metadata":{}},{"cell_type":"code","source":["# Write your code here\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\n# 1\ndef uppercase_and_replace(word, max_length=10):\n  return word.upper().replace('S', '$').replace('A', '@').replace('I', '!').replace('O', '0')[:min(len(word),max_length)] if word else None\n\n# 2\nudf_upper_and_replace = F.udf(uppercase_and_replace, T.StringType())\n\n# 3\ndf_pets\\\n  .withColumn('nickname_upper', udf_upper_and_replace(F.col('nickname')))\\\n  .withColumn('color_upper', udf_upper_and_replace(\"color\"))\\\n  .withColumn('color_upper_trimmed', udf_upper_and_replace(\"color\", F.lit(2)))\\\n  .show()\n\n# Option 2: Less Steps using a Decorator\n# from pyspark.sql.functions import udf\n# @udf('string', 'int')  # The input types\n# def uppercase_and_replace(word, max_length=10):\n#   return word.upper().replace('S', '$').replace('A', '@').replace('I', '!').replace('O', '0')[:min(len(word),max_length)] if word else None"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["## Task 8 - DataFrame to RDD\nSometimes you will need to convert a Dataframe into an RDD to do some extra manipulations that can't be done with SQL functions.\n\nIn this task, you'll have to:\n1. Convert the Pets Dataframe created in Task 4 into an RDD.\n2. Extract the `'age'` column from each `Row`. Hint: Values can be extracted from a `Row` object like you extract values from a Python Dict. i.e: `Row(a=1, b=2, c=3)['a'] returns '2'`.\n3. Multiply the age value by itself with a `map` function.\n4. Use the `reduce` function to sum the values from the previous step."],"metadata":{}},{"cell_type":"code","source":["# Write your code here\nfrom operator import add\n\nrdd_pets = df_pets.rdd\nrdd_pets\\\n      .map(lambda r: r['age'])\\\n      .map(lambda v: v*v)\\\n      .reduce(add)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## Task 9 - RDD to DataFrame\n\nSometimes you'll also need to convert an RDD into a DataFrame.\n\nBelow you are given an RDD. Your job is to convert it into a DataFrame with the following schema:\n\n`id, breed_id, nickname, birthday, age, color, weight`"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nrdd = sc.parallelize(\n        ['1,1,\"King\",\"2014-11-22 12:30:31\",5,\"brown\",10.0'\n        ,'2,3,\"Argus\",\"2016-11-22 10:05:10\",10,\"black\",5.5'\n        ,'3,1,\"Chewie\",\"2016-11-22 10:05:10\",15,\"green\",12.0'\n        ,'3,2,\"Maple\",\"2018-11-22 10:05:10\",17,\"white\",3.4'\n        ,'4,2,\"Coby\",\"2019-01-01 10:05:10\",13,\"yellow\",10.0']\n)\n\n# Write your code here\n\n# Option 1\ndf_pets_from_rdd = rdd\\\n                    .map(lambda s: s.split(','))\\\n                    .toDF([\"id\", \"breed_id\", \"nickname\", \"birthday\", \"age\", \"color\", \"weight\"])\n\n# Option 2\nfrom pyspark.sql import Row\ndf_pets_from_rdd = rdd\\\n                    .map(lambda s: s.split(','))\\\n                    .map(lambda l: Row(id=l[0], breed_id=l[1], nickname=l[2], birthday=l[3], age=l[4], color=l[5], weight=l[6]))\\\n                    .toDF()\n\ndf_pets_from_rdd.show()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["## Run SQL queries\nIn PySpark, you can issues SQL queries instead of using SparkSQL.\n\nTo do that you must register the table so it is accesible via SQL Context and then a query can be executed, let's walk over the following example:"],"metadata":{}},{"cell_type":"markdown","source":["In Task 1, you loaded `\"/databricks-datasets/samples/population-vs-price/data_geo.csv\"` into a Dataframe. After doing that, you'll need to register it:"],"metadata":{}},{"cell_type":"code","source":["df = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(\"/databricks-datasets/samples/population-vs-price/data_geo.csv\")\n\ndf.createOrReplaceTempView(\"data_geo\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["You can now query the table by using sql queries, the first option is using spark:"],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"select `State Code`, `2015 median sales price` from data_geo\").show(5)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["In databricks you can also issue sql queries by defining the cell as an SQL one, just write `%sql` at the top of the cell and then the query.\n\nAn additional benefit of using the Databricks is that you can quickly view this data with a number of embedded visualizations. Click the down arrow next to the Chart Button to display a list of visualization types.\nThen, select the Map icon to create a map visualization of the sale price SQL query below:"],"metadata":{}},{"cell_type":"code","source":["%sql\nselect `State Code`, `2015 median sales price` from data_geo\n-- Use the 'Map' visualization"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%sql\nselect City, `2014 Population estimate` from data_geo where `State Code` = 'WA' limit 3;\n-- Use the 'Bar' visualization"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["%sql\nselect City, `2014 Population estimate`/1000 as `2014 Population Estimate (1000s)`, `2015 median sales price` as `2015 Median Sales Price (1000s)` from data_geo order by `2015 median sales price` desc limit 10;\n-- Use the 'Pie' visualization, the Click on the 'Plot Options' and select `2015 Median Sales Price (1000s)` and `2014 Population Estimate (1000s)` as the values. Enlarge the graph."],"metadata":{},"outputs":[],"execution_count":28}],"metadata":{"name":"01_SpakSQL_GettinStarted_solution","notebookId":1226523811394355},"nbformat":4,"nbformat_minor":0}
