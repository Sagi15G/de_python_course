{"cells":[{"cell_type":"markdown","source":["# Exercise #2 - Popular POI\nYou are given two datasets (Upload them to Databricks):\n* Pickup/Dropoff locations of taxi rides in New York - `rides_small_ds.csv`\n* POI (Points of interest) of New York city - `poi_small_ds.csv`\n\nYour goal is to find the most popular places visited by people during morning and evening hours (morning is defined by range 6AM - 11AM, while evening is 17PM - 23PM).\n\nWe assume that a place was visited if itâ€™s located within 500 meters of pick up / drop off location.\n\nPay attention, even though the input files are 'small', you can end up with lots of data depending on the operations you make, resulting in long waiting times (~5 minutes per job).\n\n### Haversine Formula\nThe haversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes. You should use it to find the distance in meters between two Geo locations.\n\nA method that implements it is already defined below. You can use it as a UDF or to check the algorithm and implement it in any other way."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import functions as F\nimport math\n\ndef haversine_formula(lat1, lon1, lat2, lon2):\n    \"\"\"\n    Return the distance between two coordinates, in meters.\n    \"\"\"\n    lat1 = math.pi / 180.0 * lat1\n    lon1 = math.pi / 180.0 * lon1\n    lat2 = math.pi / 180.0 * lat2\n    lon2 = math.pi / 180.0 * lon2\n    radius = 6371  # km\n    meters = 1000\n\n    # Use the haversine formula:\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = math.pow(math.sin(dlat/2),2) + math.cos(lat1) * math.cos(lat2) * math.pow(math.sin(dlon/2),2)\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n    meters = radius * c * meters\n    return meters"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Write your code here\ndf_rides = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"FileStore/tables/rides_small_ds.csv\")\ndf_poi = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/FileStore/tables/poi_small_ds.csv\").withColumn('Lat', F.col(\"Lat\").cast(\"double\"))\n\ndf_rides.show(5)\ndf_poi.show(5)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Format the date and extract only the event that are on the morning or evening range\ndf_morning_event_rides = df_rides\\\n                          .select(F.to_timestamp(F.col(\"Date/Time\"), \"MM/dd/yyyy HH:mm:ss\").alias(\"arrival_time\"), F.col(\"Lat\"), F.col(\"Lon\"))\\\n                          .withColumn(\"arrival_hour\", F.hour(F.col(\"arrival_time\")))\\\n                          .where(\"(arrival_hour between 6 and 11) or (arrival_hour between 17 and 23)\")\ndf_morning_event_rides.show(5)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Solution #1 - Use a UDF for the haversine_formula - Takes ~45 seconds\nfrom pyspark.sql import types as T\nudf_haversine_formula = F.udf(haversine_formula)\n\n# Attach to each POI all drop offs and pickups that are less than 100 meters apart, using the UDF.\n# df_combined = df_morning_event_rides\\\n#                 .crossJoin(df_poi)\\\n#                 .where(udf_haversine_formula(df_morning_event_rides[\"Lat\"], df_morning_event_rides[\"Lon\"], df_poi[\"Lat\"], df_poi[\"Lon\"]) < 300)\\\n#                 .cache()\n# df_combined.show(5)\ndf_combined = df_morning_event_rides\\\n                .crossJoin(df_poi)\\\n                .withColumn('distance', udf_haversine_formula(df_morning_event_rides[\"Lat\"], df_morning_event_rides[\"Lon\"], df_poi[\"Lat\"], df_poi[\"Lon\"]))\\\n                .filter(F.col('distance') < 300)\\\n                .cache()\ndf_combined.show(5)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Solution #2 - Implement the Haversine Formula with SparkSQL's functions to improve the performance - Takes ~2 seconds\n# To understand why UDFs in Python are not always the best option, read here: https://stackoverflow.com/questions/38296609/spark-functions-vs-udf-performance\n\npi = math.pi\nradius = 6371  # km\nmeters = 1000\n\n# Prepare the two main dataframes\ndf_morning_event_rides_hav = df_morning_event_rides\\\n                              .withColumn(\"lat_hav_rides\",(pi / 180.0 * F.col(\"Lat\")))\\\n                              .withColumn(\"lon_hav_rides\", (pi / 180.0 * F.col(\"Lon\")))\n\ndf_poi_hav = df_poi\\\n              .withColumn(\"lat_hav_poi\", (pi / 180.0 * F.col(\"Lat\")))\\\n              .withColumn(\"lon_hav_poi\", (pi / 180.0 * F.col(\"Lon\")))\n\n# Crossjoin and apply haversine formula\ndf_combined = df_morning_event_rides_hav\\\n                  .crossJoin(df_poi_hav)\\\n                  .withColumn('dlon', F.col(\"lon_hav_poi\") - F.col(\"lon_hav_rides\"))\\\n                  .withColumn('dlat', F.col(\"lat_hav_poi\") - F.col(\"lat_hav_rides\"))\\\n                  .withColumn('a', F.pow(F.sin(F.col('dlat')/2),2)+ F.cos(F.col(\"lat_hav_rides\"))*F.cos(F.col(\"lat_hav_poi\"))*F.pow((F.sin(F.col('dlon')/2)),2))\\\n                  .withColumn('c', 2 * F.atan2(F.sqrt(F.col('a')), F.sqrt(1-F.col('a'))))\\\n                  .withColumn('distance', radius * meters * F.col('c'))\\\n                  .filter(F.col('distance') < 500)\\\n                  .cache()\ndf_combined.show(5)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Sort the most popular POIs\n\n# Option 1 - With SparkSQL\ndf_sorted = df_combined.groupby('Name').count().sort(\"count\", ascending=False).withColumnRenamed('count', 'num_of_visits')\n\n# option 2 - With SparkSQL Context\ndf_combined.createOrReplaceTempView(\"combined\")\ndf_sorted = spark.sql(\"select Name, count(*) as num_of_visits from combined group by Name order by 2 desc\")\n\ndf_sorted.show()"],"metadata":{},"outputs":[],"execution_count":7}],"metadata":{"name":"02_SparkSQL_PopularPOI_solution","notebookId":860674011339152},"nbformat":4,"nbformat_minor":0}
